library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("caret")
install.packages("AppliedPredictiveModeling")
# question 1
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength,
p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
featurePlot(x = training[,c('Cement',
'BlastFurnaceSlag',
'FlyAsh',
'Water',
'Superplasticizer',
'CoarseAggregate',
'FineAggregate', 'Age')],
y = training$CompressiveStrength )
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength,
p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(Superplasticizer, data=training)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
set.seed(3433)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis,
p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh = 0.8, outcome = training$diagnosis)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
set.seed(3433)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis,
p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh = 0.8, outcome = training$diagnosis)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ss, testing$diagnosis, method='glm')
model2 <- preProcess(ss, method='pca', thresh = 0.8, outcome = testing$diagnosis)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ss, testing$diagnosis, method='glm')
model2 <- preProcess(ss, method='pca', thresh = 0.8, outcome = testing$diagnosis)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ss, testing$diagnosis, method='glm')
model2 <- preProcess(ss, method='pca', thresh = 0.8, outcome = testing$diagnosis)
View(predictors)
install.packages("rpart.plot")
install.packages("rattle")
install.packages("randomForest")
install.packages("e1")
install.packages("e1071")
## References
install.packages("lattice")
install.packages("ggplot2")
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
# predict test
predict1 <- predict(fit1, newdata = vowel.test)
predict2 <- predict(fit2, newdata = vowel.test)
# combine predictions
DF_combined <- data.frame(predict1, predict2, y = vowel.test$y)
fit_combined <- train(y ~ ., data = DF_combined, method = "gam")
predict3 <- predict(fit_combined, newdata = vowel.test)
# confusion matrixes
c1 <- confusionMatrix(predict1, vowel.test$y)
c2 <- confusionMatrix(predict2, vowel.test$y)
c3 <- confusionMatrix(predict3, DF_combined$y)
# combining didn't give right result?
install.packages("ElemStatLearn")
# combining didn't give right result?
c1
print (c1)
print(c1)
c1
c3 <<- confusionMatrix(predict3, DF_combined$y)
library(ElemStatLearn)
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
# predict test
predict1 <- predict(fit1, newdata = vowel.test)
predict2 <- predict(fit2, newdata = vowel.test)
# combine predictions
DF_combined <- data.frame(predict1, predict2, y = vowel.test$y)
fit_combined <- train(y ~ ., data = DF_combined, method = "gam")
predict3 <- predict(fit_combined, newdata = vowel.test)
# confusion matrixes
c1 <- confusionMatrix(predict1, vowel.test$y)
c2 <- confusionMatrix(predict2, vowel.test$y)
c3 <- confusionMatrix(predict3, DF_combined$y)
# combining didn't give right result?
c3
c1
c2
c3
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Set the seed to 62433 and predict diagnosis with all the other variables using
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis
# ("lda") model. Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set? Is it better or worse than
# each of the individual predictions?
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
# predict test
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)
# combine predictions
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
# confusion matrixes
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 233 and fit a lasso model to predict Compressive Strength.
# Which variable is the last coefficient to be set to zero as the penalty increases?
# (Hint: it may be useful to look up ?plot.enet).
library(caret)
set.seed(233)
fit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 233 and fit a lasso model to predict Compressive Strength.
# Which variable is the last coefficient to be set to zero as the penalty increases?
# (Hint: it may be useful to look up ?plot.enet).
library(caret)
set.seed(233)
fit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
# Since we are interested in the shrinkage of coefficients as the penalty(lambda) increases, "
# penalty" looks promising for an xvar argument value.
plot.enet(fit$finalModel, xvar = "penalty", use.color = TRUE)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 233 and fit a lasso model to predict Compressive Strength.
# Which variable is the last coefficient to be set to zero as the penalty increases?
# (Hint: it may be useful to look up ?plot.enet).
library(caret)
set.seed(233)
fit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
# Since we are interested in the shrinkage of coefficients as the penalty(lambda) increases, "
# penalty" looks promising for an xvar argument value.
plot.enet(fit$finalModel, xvar = "penalty", use.color = TRUE)
#Load the data on the number of visitors to the instructors blog from here:
# https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv
library(lubridate) # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
# Fit a model using the bats() function in the forecast package to the training
# time series. Then forecast this model for the remaining time points. For how
# many of the testing points is the true value within the 95% prediction
# interval bounds?
library(forecast)
library(quantmod)
# fit a model
fit <- bats(tstrain)
# check how long the test set is, so you can predict beyond trainign
h <- dim(testing)[1]
# forecast the model for remaining time points
fcast <- forecast(fit, level = 95, h = h)
# get the accuracy
accuracy(fcast, testing$visitsTumblr)
# check what percentage of times that the actual number of visitors was within
# 95% confidence interval
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
gaData <- read.csv("C:\\Users\\Rai\\AppData\\Local\\Temp\\RtmpkzbnfZ\\data10241a8236")
View(gaData)
library(lubridate) # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
# Fit a model using the bats() function in the forecast package to the training
# time series. Then forecast this model for the remaining time points. For how
# many of the testing points is the true value within the 95% prediction
# interval bounds?
library(forecast)
library(quantmod)
# fit a model
fit <- bats(tstrain)
# check how long the test set is, so you can predict beyond trainign
h <- dim(testing)[1]
# forecast the model for remaining time points
fcast <- forecast(fit, level = 95, h = h)
# get the accuracy
accuracy(fcast, testing$visitsTumblr)
# check what percentage of times that the actual number of visitors was within
# 95% confidence interval
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
install.packages("lubridate")
library(lubridate) # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
# Fit a model using the bats() function in the forecast package to the training
# time series. Then forecast this model for the remaining time points. For how
# many of the testing points is the true value within the 95% prediction
# interval bounds?
library(forecast)
library(quantmod)
# fit a model
fit <- bats(tstrain)
# check how long the test set is, so you can predict beyond trainign
h <- dim(testing)[1]
# forecast the model for remaining time points
fcast <- forecast(fit, level = 95, h = h)
# get the accuracy
accuracy(fcast, testing$visitsTumblr)
# check what percentage of times that the actual number of visitors was within
# 95% confidence interval
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
install.packages("forecast")
install.packages("quantmod")
library(lubridate) # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
# Fit a model using the bats() function in the forecast package to the training
# time series. Then forecast this model for the remaining time points. For how
# many of the testing points is the true value within the 95% prediction
# interval bounds?
library(forecast)
library(quantmod)
# fit a model
fit <- bats(tstrain)
# check how long the test set is, so you can predict beyond trainign
h <- dim(testing)[1]
# forecast the model for remaining time points
fcast <- forecast(fit, level = 95, h = h)
# get the accuracy
accuracy(fcast, testing$visitsTumblr)
# check what percentage of times that the actual number of visitors was within
# 95% confidence interval
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
dat
gaData <- read.csv("C:\\Users\\Rai\\AppData\\Local\\Temp\\RtmpkzbnfZ\\data10246ccd941")
View(gaData)
library(lubridate) # For year() function below
gaData <- read.csv("C:\\Users\\Rai\\AppData\\Local\\Temp\\RtmpkzbnfZ\\data10246ccd941")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
# Fit a model using the bats() function in the forecast package to the training
# time series. Then forecast this model for the remaining time points. For how
# many of the testing points is the true value within the 95% prediction
# interval bounds?
library(forecast)
library(quantmod)
# fit a model
fit <- bats(tstrain)
# check how long the test set is, so you can predict beyond trainign
h <- dim(testing)[1]
# forecast the model for remaining time points
fcast <- forecast(fit, level = 95, h = h)
# get the accuracy
accuracy(fcast, testing$visitsTumblr)
# check what percentage of times that the actual number of visitors was within
# 95% confidence interval
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 325 and fit a support vector machine using the e1071 package
# to predict Compressive Strength using the default settings. Predict on the
# testing set. What is the RMSE?
set.seed(325)
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
install.packages("kernlab")
install.packages("kernlab")
install.packages("kernlab")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 325 and fit a support vector machine using the e1071 package
# to predict Compressive Strength using the default settings. Predict on the
# testing set. What is the RMSE?
set.seed(325)
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
install.packages(pkg[!good])
install.packages("kernlab")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 325 and fit a support vector machine using the e1071 package
# to predict Compressive Strength using the default settings. Predict on the
# testing set. What is the RMSE?
set.seed(325)
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
prediction <- predict(fit, testing)
accuracy(prediction, testing$CompressiveStrength)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 325 and fit a support vector machine using the e1071 package
# to predict Compressive Strength using the default settings. Predict on the
# testing set. What is the RMSE?
set.seed(325)
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
prediction <- predict(fit, testing)
acuracy(prediction, testing$CompressiveStrength)
install.packages("e1071")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 325 and fit a support vector machine using the e1071 package
# to predict Compressive Strength using the default settings. Predict on the
# testing set. What is the RMSE?
set.seed(325)
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
prediction <- predict(fit, testing)
accuracy(prediction, testing$CompressiveStrength)
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot, s = slider(0, 2, step = 0.1))
runApp()
library(manipulate)
manipulate(plot(1:x), x = slider(1, 100))
library(manipulate)
> myPlot <- function(s) {
+     plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
+     abline(0, s)
+ };
manipulate(myPlot, s = slider(0, 2, step = 0.1))
library(manipulate)
> myPlot <- function(s) {
+     plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
+     abline(0, s)
+ };
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
setwd("C:/Curso R/IndMasCorp/IMC")
